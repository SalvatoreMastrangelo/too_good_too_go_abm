{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591316f6",
   "metadata": {},
   "source": [
    "\n",
    "# ü•ñ TGTG ‚Äî Fast Simulation Notebook (Scenario Sweeps + Robust Answers)\n",
    "\n",
    "This notebook uses the compiled simulator in `tgtg_fast.py` to run **many scenarios quickly**, **parallelise** evaluations, and generate evidence for two research questions:\n",
    "\n",
    "1) **Does TGTG increase or decrease production?**  \n",
    "   - Tested **robustly** across *different stochastic demand specifications*.\n",
    "\n",
    "2) **How do basic hyperparameters shift outcomes?**  \n",
    "   - number of goods \\(L\\)  \n",
    "   - relative prices / margins (\\(\r",
    "ho/\\chi\\), \\(\tau/\r",
    "ho\\))  \n",
    "   - volatility & persistence of demand (Beta concentration or AR(1) logit volatility)\n",
    "\n",
    "**What you get out**\n",
    "- A clean *baseline vs TGTG* comparison of **production**, **waste**, and **profit**.\n",
    "- A sweep over hyperparameters with visualizations of **Œî production** and **Œî waste**.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions & mapping to your model\n",
    "- Regular sales occur at unit price \\(\r",
    "ho\\).\n",
    "- TGTG sales are modelled as **per-unit salvage price** \\(\tau\\) on the reserved share \\(b\\).\n",
    "  - If you instead want ‚Äúbags‚Äù of size \\(k\\) sold at price \\(\tau\\) per bag, see the note at the end (small change).\n",
    "- Baker chooses:\n",
    "  - production vector \\(q \\in \\mathbb{Z}_{\\ge 0}^L\\)\n",
    "  - reserved share \\(b \\in [0,1]\\)\n",
    "- Fitness is risk-adjusted profit across days:\n",
    "  \\[\n",
    "  \text{fitness}=D(\bar{\\pi}-\\gamma \\sigma_{\\pi})\n",
    "  \\]\n",
    "- Demand is via **visit probability** \\(\u0007lpha_t\\) per day (stochastic path).\n",
    "\n",
    "> Tip: run the notebook top-to-bottom once to compile Numba, then scenario sweeps become much faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you placed tgtg_fast.py in the same folder as this notebook, this works.\n",
    "# In this environment it's in /mnt/data; we add that to sys.path.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"/mnt/data\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tgtg_fast import (\n",
    "    EnvironmentConfig, BakerAgent,\n",
    "    generate_preferences,\n",
    "    make_common_random_draws,\n",
    "    alpha_path_constant, alpha_path_beta_shocks, alpha_path_logit_ar1,\n",
    "    simulate_epoch_fast\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64c84b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Core helpers: evaluation, evolutionary optimizer, and out-of-sample testing\n",
    "\n",
    "We use a lightweight evolutionary optimizer to approximate the baker‚Äôs best-response (optimal \\(q\\) and \\(b\\)).  \n",
    "This is modular so you can later swap in Bayesian optimisation / CMA-ES / etc.\n",
    "\n",
    "### Why common random numbers?\n",
    "At each generation we evaluate all candidates on the **same** random draws (`perm`, `visit_u`, `walk_u`) so selection is less noisy and results are more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8238946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_agent(agent: BakerAgent, q_max: int) -> BakerAgent:\n",
    "    q = np.clip(np.floor(agent.q), 0, q_max).astype(np.int64)\n",
    "    b = float(np.clip(agent.b, 0.0, 1.0))\n",
    "    return BakerAgent(q=q, b=b, gamma=float(agent.gamma))\n",
    "\n",
    "\n",
    "def init_population(P: int, L: int, gamma: float, q_max: int, rng: np.random.Generator) -> list[BakerAgent]:\n",
    "    pop = []\n",
    "    for _ in range(P):\n",
    "        q = rng.integers(low=0, high=q_max + 1, size=L, dtype=np.int64)\n",
    "        b = rng.random()\n",
    "        pop.append(BakerAgent(q=q, b=float(b), gamma=float(gamma)))\n",
    "    return pop\n",
    "\n",
    "\n",
    "def mutate(agent: BakerAgent, q_sigma: float, b_sigma: float, q_max: int, rng: np.random.Generator) -> BakerAgent:\n",
    "    q = agent.q.astype(np.float64) + rng.normal(0.0, q_sigma, size=agent.q.shape[0])\n",
    "    b = agent.b + float(rng.normal(0.0, b_sigma))\n",
    "    child = BakerAgent(q=q, b=b, gamma=agent.gamma)\n",
    "    return clip_agent(child, q_max=q_max)\n",
    "\n",
    "\n",
    "def evaluate_population_parallel(\n",
    "    population: list[BakerAgent],\n",
    "    prefs: np.ndarray,\n",
    "    env: EnvironmentConfig,\n",
    "    alpha_path: np.ndarray,\n",
    "    perm: np.ndarray,\n",
    "    visit_u: np.ndarray,\n",
    "    walk_u: np.ndarray,\n",
    "    max_workers: int | None = None\n",
    ") -> tuple[np.ndarray, list[dict]]:\n",
    "    \"\"\"Parallel evaluation of the population using threads.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, os.cpu_count() or 1)\n",
    "\n",
    "    metrics_list = [None] * len(population)\n",
    "\n",
    "    def _eval_one(idx: int):\n",
    "        m = simulate_epoch_fast(population[idx], prefs, env, alpha_path, perm, visit_u, walk_u)\n",
    "        return idx, m\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, i) for i in range(len(population))]\n",
    "        for f in as_completed(futures):\n",
    "            idx, m = f.result()\n",
    "            metrics_list[idx] = m\n",
    "\n",
    "    fitness = np.array([m[\"fitness\"] for m in metrics_list], dtype=np.float64)\n",
    "    return fitness, metrics_list\n",
    "\n",
    "\n",
    "def evolutionary_optimize(\n",
    "    env: EnvironmentConfig,\n",
    "    demand_spec: dict,\n",
    "    *,\n",
    "    gamma: float = 0.8,\n",
    "    allow_tgtg: bool = True,\n",
    "    preference_mode: str = \"random\",\n",
    "    seed: int = 0,\n",
    "    D: int = 30,\n",
    "    P: int = 80,\n",
    "    G: int = 30,\n",
    "    elite_frac: float = 0.20,\n",
    "    q_max: int = 120,\n",
    "    q_sigma: float = 10.0,\n",
    "    b_sigma: float = 0.08,\n",
    "    n_workers: int | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    demand_spec examples:\n",
    "      {\"kind\":\"constant\", \"alpha\":0.35}\n",
    "      {\"kind\":\"beta\", \"alpha_mean\":0.35, \"concentration\":20.0}\n",
    "      {\"kind\":\"ar1\", \"alpha0\":0.35, \"phi\":0.8, \"sigma\":0.6}\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    prefs = generate_preferences(env.N, env.L, mode=preference_mode, rng=rng)\n",
    "\n",
    "    pop = init_population(P, env.L, gamma=gamma, q_max=q_max, rng=rng)\n",
    "    if not allow_tgtg:\n",
    "        for a in pop:\n",
    "            a.b = 0.0\n",
    "\n",
    "    elite_n = max(2, int(P * elite_frac))\n",
    "    history = []\n",
    "\n",
    "    def make_alpha_path(rng_local: np.random.Generator) -> np.ndarray:\n",
    "        k = demand_spec[\"kind\"]\n",
    "        if k == \"constant\":\n",
    "            return alpha_path_constant(D, demand_spec[\"alpha\"])\n",
    "        if k == \"beta\":\n",
    "            return alpha_path_beta_shocks(D, demand_spec[\"alpha_mean\"], demand_spec[\"concentration\"], rng_local)\n",
    "        if k == \"ar1\":\n",
    "            return alpha_path_logit_ar1(D, demand_spec[\"alpha0\"], demand_spec[\"phi\"], demand_spec[\"sigma\"], rng_local)\n",
    "        raise ValueError(f\"Unknown demand spec kind: {k}\")\n",
    "\n",
    "    best_agent = None\n",
    "    best_fit = -1e18\n",
    "    best_metrics = None\n",
    "\n",
    "    # Warm-up compile once\n",
    "    perm0, visit0, walk0 = make_common_random_draws(D, env.N, env.L, seed=seed + 10_000)\n",
    "    alpha0 = make_alpha_path(np.random.default_rng(seed + 20_000))\n",
    "    _ = simulate_epoch_fast(pop[0], prefs, env, alpha0, perm0, visit0, walk0)\n",
    "\n",
    "    for g in range(G):\n",
    "        perm, visit_u, walk_u = make_common_random_draws(D, env.N, env.L, seed=seed + 1_000_000 + g)\n",
    "        alpha_path = make_alpha_path(np.random.default_rng(seed + 2_000_000 + g))\n",
    "\n",
    "        fitness, metrics_list = evaluate_population_parallel(\n",
    "            pop, prefs, env, alpha_path, perm, visit_u, walk_u, max_workers=n_workers\n",
    "        )\n",
    "\n",
    "        order = np.argsort(fitness)[::-1]\n",
    "        elites = [pop[i] for i in order[:elite_n]]\n",
    "        elite_metrics = [metrics_list[i] for i in order[:elite_n]]\n",
    "\n",
    "        if float(fitness[order[0]]) > best_fit:\n",
    "            best_fit = float(fitness[order[0]])\n",
    "            best_agent = elites[0]\n",
    "            best_metrics = elite_metrics[0]\n",
    "\n",
    "        history.append({\n",
    "            \"gen\": g,\n",
    "            \"best_fitness\": float(fitness[order[0]]),\n",
    "            \"mean_fitness\": float(np.mean(fitness)),\n",
    "            \"best_production\": float(metrics_list[order[0]][\"production\"]),\n",
    "            \"best_waste\": float(metrics_list[order[0]][\"waste\"]),\n",
    "            \"best_b\": float(pop[order[0]].b),\n",
    "        })\n",
    "\n",
    "        new_pop = elites.copy()\n",
    "        while len(new_pop) < P:\n",
    "            parent = elites[rng.integers(0, elite_n)]\n",
    "            child = mutate(parent, q_sigma=q_sigma, b_sigma=b_sigma, q_max=q_max, rng=rng)\n",
    "            if not allow_tgtg:\n",
    "                child.b = 0.0\n",
    "            new_pop.append(child)\n",
    "        pop = new_pop\n",
    "\n",
    "    return {\n",
    "        \"best_agent\": best_agent,\n",
    "        \"best_fit\": best_fit,\n",
    "        \"best_metrics_train\": best_metrics,\n",
    "        \"prefs\": prefs,\n",
    "        \"history\": pd.DataFrame(history),\n",
    "        \"env\": env,\n",
    "        \"demand_spec\": demand_spec,\n",
    "        \"seed\": seed,\n",
    "        \"D\": D,\n",
    "    }\n",
    "\n",
    "\n",
    "def out_of_sample_eval(\n",
    "    agent: BakerAgent,\n",
    "    prefs: np.ndarray,\n",
    "    env: EnvironmentConfig,\n",
    "    demand_spec: dict,\n",
    "    *,\n",
    "    seed: int,\n",
    "    D: int = 30,\n",
    "    reps: int = 30\n",
    ") -> pd.DataFrame:\n",
    "    def make_alpha_path(rng_local: np.random.Generator) -> np.ndarray:\n",
    "        k = demand_spec[\"kind\"]\n",
    "        if k == \"constant\":\n",
    "            return alpha_path_constant(D, demand_spec[\"alpha\"])\n",
    "        if k == \"beta\":\n",
    "            return alpha_path_beta_shocks(D, demand_spec[\"alpha_mean\"], demand_spec[\"concentration\"], rng_local)\n",
    "        if k == \"ar1\":\n",
    "            return alpha_path_logit_ar1(D, demand_spec[\"alpha0\"], demand_spec[\"phi\"], demand_spec[\"sigma\"], rng_local)\n",
    "        raise ValueError(f\"Unknown demand spec kind: {k}\")\n",
    "\n",
    "    rows = []\n",
    "    for j in range(reps):\n",
    "        perm, visit_u, walk_u = make_common_random_draws(D, env.N, env.L, seed=seed + 100_000 + j)\n",
    "        alpha_path = make_alpha_path(np.random.default_rng(seed + 200_000 + j))\n",
    "        m = simulate_epoch_fast(agent, prefs, env, alpha_path, perm, visit_u, walk_u)\n",
    "        m = dict(m)\n",
    "        m[\"rep\"] = j\n",
    "        rows.append(m)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc0407",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Single-scenario: baseline vs TGTG across multiple demand specs\n",
    "\n",
    "We compare:\n",
    "- **Baseline**: force \\(b=0\\) (no TGTG)\n",
    "- **TGTG**: optimise both \\(q\\) and \\(b\\)\n",
    "\n",
    "and we do this across different stochastic demand specs for robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env0 = EnvironmentConfig(\n",
    "    N=600,\n",
    "    L=6,\n",
    "    r=0.35,\n",
    "    chi=1.0,\n",
    "    rho=2.5,\n",
    "    tau=0.8\n",
    ")\n",
    "gamma0 = 0.8\n",
    "D0 = 30\n",
    "\n",
    "demand_specs = [\n",
    "    {\"kind\":\"constant\", \"alpha\":0.35, \"label\":\"constant Œ±=0.35\"},\n",
    "    {\"kind\":\"beta\", \"alpha_mean\":0.35, \"concentration\":20.0, \"label\":\"beta shocks (conc=20)\"},\n",
    "    {\"kind\":\"beta\", \"alpha_mean\":0.35, \"concentration\":5.0, \"label\":\"beta shocks (conc=5, high vol)\"},\n",
    "    {\"kind\":\"ar1\", \"alpha0\":0.35, \"phi\":0.8, \"sigma\":0.6, \"label\":\"logit-AR1 (œÜ=0.8, œÉ=0.6)\"},\n",
    "]\n",
    "\n",
    "# Optimiser budget\n",
    "P = 80\n",
    "G = 30\n",
    "\n",
    "seed_base = 1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8851ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "trained = {}\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for k, spec in enumerate(demand_specs):\n",
    "    print(f\"\\n=== Demand spec: {spec['label']} ===\")\n",
    "\n",
    "    out_tgtg = evolutionary_optimize(\n",
    "        env0, spec,\n",
    "        gamma=gamma0, allow_tgtg=True,\n",
    "        preference_mode=\"correlated\",\n",
    "        seed=seed_base + 10*k,\n",
    "        D=D0, P=P, G=G,\n",
    "        n_workers=None\n",
    "    )\n",
    "\n",
    "    out_base = evolutionary_optimize(\n",
    "        env0, spec,\n",
    "        gamma=gamma0, allow_tgtg=False,\n",
    "        preference_mode=\"correlated\",\n",
    "        seed=seed_base + 10*k,\n",
    "        D=D0, P=P, G=G,\n",
    "        n_workers=None\n",
    "    )\n",
    "\n",
    "    trained[(spec[\"label\"], \"tgtg\")] = out_tgtg\n",
    "    trained[(spec[\"label\"], \"base\")] = out_base\n",
    "\n",
    "    oos_tgtg = out_of_sample_eval(out_tgtg[\"best_agent\"], out_tgtg[\"prefs\"], env0, spec, seed=seed_base + 10*k + 1, D=D0, reps=25)\n",
    "    oos_base = out_of_sample_eval(out_base[\"best_agent\"], out_base[\"prefs\"], env0, spec, seed=seed_base + 10*k + 1, D=D0, reps=25)\n",
    "\n",
    "    def summarize(df):\n",
    "        return {\n",
    "            \"production_mean\": df[\"production\"].mean(),\n",
    "            \"waste_mean\": df[\"waste\"].mean(),\n",
    "            \"profit_mean\": df[\"mean_profit\"].mean(),\n",
    "        }\n",
    "\n",
    "    s_t = summarize(oos_tgtg)\n",
    "    s_b = summarize(oos_base)\n",
    "\n",
    "    results.append({\n",
    "        \"demand_spec\": spec[\"label\"],\n",
    "        \"prod_base\": s_b[\"production_mean\"],\n",
    "        \"prod_tgtg\": s_t[\"production_mean\"],\n",
    "        \"delta_prod\": s_t[\"production_mean\"] - s_b[\"production_mean\"],\n",
    "        \"waste_base\": s_b[\"waste_mean\"],\n",
    "        \"waste_tgtg\": s_t[\"waste_mean\"],\n",
    "        \"delta_waste\": s_t[\"waste_mean\"] - s_b[\"waste_mean\"],\n",
    "        \"profit_base\": s_b[\"profit_mean\"],\n",
    "        \"profit_tgtg\": s_t[\"profit_mean\"],\n",
    "        \"delta_profit\": s_t[\"profit_mean\"] - s_b[\"profit_mean\"],\n",
    "        \"b_star_tgtg\": float(out_tgtg[\"best_agent\"].b),\n",
    "        \"prod_per_day_tgtg\": s_t[\"production_mean\"]/D0,\n",
    "        \"prod_per_day_base\": s_b[\"production_mean\"]/D0,\n",
    "    })\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone in {elapsed:.1f}s\")\n",
    "df_comp = pd.DataFrame(results).sort_values(\"delta_prod\", ascending=False)\n",
    "df_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visual answer to Q1: does TGTG increase or decrease production? (by demand spec)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.arange(len(df_comp))\n",
    "ax.bar(x - 0.2, df_comp[\"prod_base\"]/D0, width=0.4, label=\"Baseline (b=0)\")\n",
    "ax.bar(x + 0.2, df_comp[\"prod_tgtg\"]/D0, width=0.4, label=\"TGTG (b optimized)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_comp[\"demand_spec\"], rotation=25, ha=\"right\")\n",
    "ax.set_ylabel(\"Production per day\")\n",
    "ax.set_title(\"Production: Baseline vs TGTG across demand specs\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.bar(x, df_comp[\"delta_prod\"]/D0)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_comp[\"demand_spec\"], rotation=25, ha=\"right\")\n",
    "ax.set_ylabel(\"Œî Production per day (TGTG - Baseline)\")\n",
    "ax.set_title(\"Œî Production per day by demand spec (robustness check)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68290e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supporting evidence: waste & profit effects\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.bar(x, df_comp[\"delta_waste\"]/D0)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_comp[\"demand_spec\"], rotation=25, ha=\"right\")\n",
    "ax.set_ylabel(\"Œî Waste per day (TGTG - Baseline)\")\n",
    "ax.set_title(\"Œî Waste per day by demand spec\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.bar(x, df_comp[\"delta_profit\"])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_comp[\"demand_spec\"], rotation=25, ha=\"right\")\n",
    "ax.set_ylabel(\"Œî Mean profit (TGTG - Baseline)\")\n",
    "ax.set_title(\"Œî Mean profit by demand spec (out-of-sample)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0778f7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Hyperparameter sweeps (Q2)\n",
    "\n",
    "We vary:\n",
    "- \\(L\\) (number of goods)\n",
    "- margins via \\(\r",
    "ho/\\chi\\) and discount via \\(\tau/\r",
    "ho\\)\n",
    "- demand volatility (Beta concentration)\n",
    "\n",
    "We record:\n",
    "- optimal reserved share \\(b^\\*\\)\n",
    "- Œî production (TGTG - baseline)\n",
    "- Œî waste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f73bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sweep configuration (modest defaults for interactive use)\n",
    "\n",
    "SWEEP_L = [3, 6, 10]\n",
    "MARGINS = [1.8, 2.5, 3.2]      # rho = margin * chi\n",
    "DISCOUNTS = [0.2, 0.35, 0.5]   # tau = discount * rho\n",
    "BETA_CONC = [5.0, 20.0, 100.0] # smaller => more volatile demand\n",
    "\n",
    "N_sweep = 500\n",
    "r_sweep = 0.35\n",
    "chi_sweep = 1.0\n",
    "\n",
    "P_sweep = 60\n",
    "G_sweep = 22\n",
    "D_sweep = 25\n",
    "oos_reps = 15\n",
    "\n",
    "seed_sweep = 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_one_sweep_point(args) -> dict:\n",
    "    L, margin, disc, conc, idx = args\n",
    "\n",
    "    env = EnvironmentConfig(\n",
    "        N=N_sweep, L=L, r=r_sweep,\n",
    "        chi=chi_sweep, rho=margin*chi_sweep, tau=disc*(margin*chi_sweep)\n",
    "    )\n",
    "\n",
    "    spec = {\"kind\":\"beta\", \"alpha_mean\":0.35, \"concentration\":conc, \"label\": f\"beta(conc={conc})\"}\n",
    "\n",
    "    out_t = evolutionary_optimize(env, spec, gamma=gamma0, allow_tgtg=True,\n",
    "                                 preference_mode=\"correlated\",\n",
    "                                 seed=seed_sweep + 1000*idx + 1,\n",
    "                                 D=D_sweep, P=P_sweep, G=G_sweep, n_workers=None)\n",
    "\n",
    "    out_b = evolutionary_optimize(env, spec, gamma=gamma0, allow_tgtg=False,\n",
    "                                 preference_mode=\"correlated\",\n",
    "                                 seed=seed_sweep + 1000*idx + 1,\n",
    "                                 D=D_sweep, P=P_sweep, G=G_sweep, n_workers=None)\n",
    "\n",
    "    oos_t = out_of_sample_eval(out_t[\"best_agent\"], out_t[\"prefs\"], env, spec, seed=seed_sweep + 1000*idx + 2, D=D_sweep, reps=oos_reps)\n",
    "    oos_b = out_of_sample_eval(out_b[\"best_agent\"], out_b[\"prefs\"], env, spec, seed=seed_sweep + 1000*idx + 2, D=D_sweep, reps=oos_reps)\n",
    "\n",
    "    row = {\n",
    "        \"L\": L,\n",
    "        \"margin_rho_over_chi\": margin,\n",
    "        \"discount_tau_over_rho\": disc,\n",
    "        \"beta_concentration\": conc,\n",
    "        \"b_star\": float(out_t[\"best_agent\"].b),\n",
    "        \"prod_base\": oos_b[\"production\"].mean(),\n",
    "        \"prod_tgtg\": oos_t[\"production\"].mean(),\n",
    "        \"delta_prod\": oos_t[\"production\"].mean() - oos_b[\"production\"].mean(),\n",
    "        \"waste_base\": oos_b[\"waste\"].mean(),\n",
    "        \"waste_tgtg\": oos_t[\"waste\"].mean(),\n",
    "        \"delta_waste\": oos_t[\"waste\"].mean() - oos_b[\"waste\"].mean(),\n",
    "        \"profit_base\": oos_b[\"mean_profit\"].mean(),\n",
    "        \"profit_tgtg\": oos_t[\"mean_profit\"].mean(),\n",
    "        \"delta_profit\": oos_t[\"mean_profit\"].mean() - oos_b[\"mean_profit\"].mean(),\n",
    "    }\n",
    "    return row\n",
    "\n",
    "\n",
    "points = []\n",
    "idx = 0\n",
    "for L in SWEEP_L:\n",
    "    for m in MARGINS:\n",
    "        for d in DISCOUNTS:\n",
    "            for c in BETA_CONC:\n",
    "                points.append((L, m, d, c, idx))\n",
    "                idx += 1\n",
    "\n",
    "len(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run sweep (parallel)\n",
    "\n",
    "max_workers = max(1, os.cpu_count() or 1)\n",
    "print(\"max_workers:\", max_workers)\n",
    "\n",
    "t0 = time.time()\n",
    "rows = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = [ex.submit(run_one_sweep_point, p) for p in points]\n",
    "    for j, f in enumerate(as_completed(futures), 1):\n",
    "        rows.append(f.result())\n",
    "        if j % 5 == 0:\n",
    "            print(f\"{j}/{len(points)} done\")\n",
    "\n",
    "df_sweep = pd.DataFrame(rows)\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Completed sweep in {elapsed/60:.1f} min\")\n",
    "df_sweep.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualise: demand volatility -> Œî production (averaged over prices), grouped by L\n",
    "\n",
    "df_sweep2 = df_sweep.copy()\n",
    "df_sweep2[\"delta_prod_per_day\"] = df_sweep2[\"delta_prod\"] / D_sweep\n",
    "df_sweep2[\"delta_waste_per_day\"] = df_sweep2[\"delta_waste\"] / D_sweep\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "for L in sorted(df_sweep2[\"L\"].unique()):\n",
    "    sub = df_sweep2[df_sweep2[\"L\"] == L].groupby(\"beta_concentration\")[\"delta_prod_per_day\"].mean().reset_index()\n",
    "    ax.plot(sub[\"beta_concentration\"], sub[\"delta_prod_per_day\"], marker=\"o\", label=f\"L={L}\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.set_xlabel(\"Beta concentration (log scale) ‚Äî lower = more volatile demand\")\n",
    "ax.set_ylabel(\"Mean Œî production per day\")\n",
    "ax.set_title(\"Effect of demand volatility on Œî production (averaged over prices)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# b* vs discount œÑ/œÅ\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "sub = df_sweep2.groupby(\"discount_tau_over_rho\")[\"b_star\"].mean().reset_index()\n",
    "ax.plot(sub[\"discount_tau_over_rho\"], sub[\"b_star\"], marker=\"o\")\n",
    "ax.set_xlabel(\"Discount (œÑ/œÅ)\")\n",
    "ax.set_ylabel(\"Mean optimal b*\")\n",
    "ax.set_title(\"How the TGTG discount affects optimal reserved share b*\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787946a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick \"microeconomic indications\": grouped summaries you can quote in text\n",
    "\n",
    "summary = (\n",
    "    df_sweep2\n",
    "    .groupby([\"L\", \"beta_concentration\"])[[\"delta_prod_per_day\", \"delta_waste_per_day\", \"b_star\", \"delta_profit\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values([\"L\", \"beta_concentration\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bbf7c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Reporting checklist (tight answers)\n",
    "\n",
    "### Q1 ‚Äî TGTG effect on production (robustness)\n",
    "Use `df_comp`:\n",
    "- sign and magnitude of `delta_prod`\n",
    "- whether sign is stable across demand specs\n",
    "- include `delta_waste` and `delta_profit` as supporting mechanism evidence\n",
    "\n",
    "### Q2 ‚Äî Hyperparameter effects\n",
    "Use `df_sweep` plots + `summary`:\n",
    "- How Œî production changes with demand volatility and number of goods \\(L\\)\n",
    "- How \\(b^\\*\\) changes with the discount \\(\tau/\r",
    "ho\\)\n",
    "- Optional: filter `df_sweep` to isolate one dimension (e.g., fix \\(L\\)) to make cleaner plots\n",
    "\n",
    "---\n",
    "\n",
    "## 5) If you want TGTG ‚Äúbags‚Äù (œÑ per bag, size k)\n",
    "Current implementation: per-unit salvage price \\(\tau\\) on the reserved share.\n",
    "\n",
    "To model bags:\n",
    "- choose bag size `k` units per bag\n",
    "- reserved capacity becomes `B*k`\n",
    "- revenue becomes `B * tau`\n",
    "\n",
    "This is a small change in the compiled core (`tgtg_fast.py`).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
