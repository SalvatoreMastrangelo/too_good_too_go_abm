{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a2b6cf",
   "metadata": {},
   "source": [
    "\n",
    "# TGTG — **Gamma (Risk Aversion) as Main Channel**\n",
    "## Full-factorial sweeps + interaction analysis focused on \\(\\gamma\\)\n",
    "\n",
    "This notebook makes **risk aversion \\(\\gamma\\)** the main hyperparameter channel.\n",
    "\n",
    "For each hyperparameter combination (including \\(\\gamma\\)) it:\n",
    "1) trains a **Baseline** policy (forces \\(b=0\\))  \n",
    "2) trains a **TGTG-available** policy (optimizes \\(q\\) and \\(b\\))  \n",
    "3) evaluates both **out-of-sample** and stores waste/production/profit outcomes and deltas.\n",
    "\n",
    "Then it produces **gamma-centric** plots and fits interaction models including \\(\\gamma\\).\n",
    "\n",
    "---\n",
    "\n",
    "### Objective (as implemented)\n",
    "Fitness uses daily profits \\(\\pi_t\\) over \\(D\\) days:\n",
    "\\[\n",
    "\\text{fitness} = D(\\bar{\\pi} - \\gamma \\sigma_{\\pi})\n",
    "\\]\n",
    "Higher \\(\\gamma\\) increases sensitivity to profit volatility, so TGTG can act as a risk-mitigation channel.\n",
    "\n",
    "---\n",
    "\n",
    "### Note on \\(\\tau\\)\n",
    "The simulator treats \\(\\tau\\) as a **per-unit** TGTG price (salvage value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b05f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"/mnt/data\")  # adjust if needed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "from tgtg_fast import (\n",
    "    EnvironmentConfig, BakerAgent,\n",
    "    generate_preferences,\n",
    "    make_common_random_draws,\n",
    "    alpha_path_constant, alpha_path_beta_shocks, alpha_path_logit_ar1,\n",
    "    simulate_epoch_fast\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "\n",
    "OUT_DIR = Path(\"./tgtg_gamma_grid_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33072e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Optimizer helpers ---\n",
    "def clip_agent(agent: BakerAgent, q_max: int) -> BakerAgent:\n",
    "    q = np.clip(np.floor(agent.q), 0, q_max).astype(np.int64)\n",
    "    b = float(np.clip(agent.b, 0.0, 1.0))\n",
    "    return BakerAgent(q=q, b=b, gamma=float(agent.gamma))\n",
    "\n",
    "def init_population(P: int, L: int, gamma: float, q_max: int, rng: np.random.Generator) -> list[BakerAgent]:\n",
    "    pop = []\n",
    "    for _ in range(P):\n",
    "        q = rng.integers(low=0, high=q_max + 1, size=L, dtype=np.int64)\n",
    "        b = float(rng.random())\n",
    "        pop.append(BakerAgent(q=q, b=b, gamma=float(gamma)))\n",
    "    return pop\n",
    "\n",
    "def mutate(agent: BakerAgent, q_sigma: float, b_sigma: float, q_max: int, rng: np.random.Generator) -> BakerAgent:\n",
    "    q = agent.q.astype(np.float64) + rng.normal(0.0, q_sigma, size=agent.q.shape[0])\n",
    "    b = float(agent.b + rng.normal(0.0, b_sigma))\n",
    "    return clip_agent(BakerAgent(q=q, b=b, gamma=agent.gamma), q_max=q_max)\n",
    "\n",
    "def evaluate_population_parallel_threads(\n",
    "    population: list[BakerAgent],\n",
    "    prefs: np.ndarray,\n",
    "    env: EnvironmentConfig,\n",
    "    alpha_path: np.ndarray,\n",
    "    perm: np.ndarray,\n",
    "    visit_u: np.ndarray,\n",
    "    walk_u: np.ndarray,\n",
    "    max_workers: int | None = None,\n",
    ") -> tuple[np.ndarray, list[dict]]:\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, os.cpu_count() or 1)\n",
    "    metrics_list = [None] * len(population)\n",
    "\n",
    "    def _eval_one(idx: int):\n",
    "        m = simulate_epoch_fast(population[idx], prefs, env, alpha_path, perm, visit_u, walk_u)\n",
    "        return idx, m\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_eval_one, i) for i in range(len(population))]\n",
    "        for f in as_completed(futures):\n",
    "            idx, m = f.result()\n",
    "            metrics_list[idx] = m\n",
    "\n",
    "    fitness = np.array([m[\"fitness\"] for m in metrics_list], dtype=np.float64)\n",
    "    return fitness, metrics_list\n",
    "\n",
    "def make_alpha_path(D: int, demand_spec: dict, rng: np.random.Generator) -> np.ndarray:\n",
    "    k = demand_spec[\"kind\"]\n",
    "    if k == \"constant\":\n",
    "        return alpha_path_constant(D, demand_spec[\"alpha\"])\n",
    "    if k == \"beta\":\n",
    "        return alpha_path_beta_shocks(D, demand_spec[\"alpha_mean\"], demand_spec[\"concentration\"], rng)\n",
    "    if k == \"ar1\":\n",
    "        return alpha_path_logit_ar1(D, demand_spec[\"alpha0\"], demand_spec[\"phi\"], demand_spec[\"sigma\"], rng)\n",
    "    raise ValueError(f\"Unknown demand spec kind: {k}\")\n",
    "\n",
    "def evolutionary_optimize(\n",
    "    env: EnvironmentConfig,\n",
    "    demand_spec: dict,\n",
    "    *,\n",
    "    gamma: float,\n",
    "    allow_tgtg: bool,\n",
    "    preference_mode: str,\n",
    "    seed: int,\n",
    "    D: int,\n",
    "    P: int,\n",
    "    G: int,\n",
    "    elite_frac: float,\n",
    "    q_max: int,\n",
    "    q_sigma: float,\n",
    "    b_sigma: float,\n",
    "    n_workers_eval: int | None = None,\n",
    ") -> dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    prefs = generate_preferences(env.N, env.L, mode=preference_mode, rng=rng)\n",
    "\n",
    "    pop = init_population(P, env.L, gamma=float(gamma), q_max=q_max, rng=rng)\n",
    "    if not allow_tgtg:\n",
    "        for a in pop:\n",
    "            a.b = 0.0\n",
    "\n",
    "    elite_n = max(2, int(P * elite_frac))\n",
    "\n",
    "    # warm-up compile\n",
    "    perm0, visit0, walk0 = make_common_random_draws(D, env.N, env.L, seed=seed + 10_000)\n",
    "    alpha0 = make_alpha_path(D, demand_spec, np.random.default_rng(seed + 20_000))\n",
    "    _ = simulate_epoch_fast(pop[0], prefs, env, alpha0, perm0, visit0, walk0)\n",
    "\n",
    "    best_fit = -1e18\n",
    "    best_agent = None\n",
    "    best_metrics = None\n",
    "\n",
    "    for g in range(G):\n",
    "        perm, visit_u, walk_u = make_common_random_draws(D, env.N, env.L, seed=seed + 1_000_000 + g)\n",
    "        alpha_path = make_alpha_path(D, demand_spec, np.random.default_rng(seed + 2_000_000 + g))\n",
    "\n",
    "        fitness, metrics_list = evaluate_population_parallel_threads(\n",
    "            pop, prefs, env, alpha_path, perm, visit_u, walk_u, max_workers=n_workers_eval\n",
    "        )\n",
    "\n",
    "        order = np.argsort(fitness)[::-1]\n",
    "        elites = [pop[i] for i in order[:elite_n]]\n",
    "\n",
    "        if float(fitness[order[0]]) > best_fit:\n",
    "            best_fit = float(fitness[order[0]])\n",
    "            best_agent = elites[0]\n",
    "            best_metrics = metrics_list[order[0]]\n",
    "\n",
    "        new_pop = elites.copy()\n",
    "        while len(new_pop) < P:\n",
    "            parent = elites[rng.integers(0, elite_n)]\n",
    "            child = mutate(parent, q_sigma=q_sigma, b_sigma=b_sigma, q_max=q_max, rng=rng)\n",
    "            if not allow_tgtg:\n",
    "                child.b = 0.0\n",
    "            new_pop.append(child)\n",
    "        pop = new_pop\n",
    "\n",
    "    return {\n",
    "        \"best_agent\": best_agent,\n",
    "        \"best_fit\": best_fit,\n",
    "        \"best_metrics_train\": best_metrics,\n",
    "        \"prefs\": prefs,\n",
    "        \"env\": env,\n",
    "        \"demand_spec\": demand_spec,\n",
    "        \"seed\": seed,\n",
    "        \"D\": D,\n",
    "        \"gamma\": float(gamma),\n",
    "    }\n",
    "\n",
    "def out_of_sample_eval(\n",
    "    agent: BakerAgent,\n",
    "    prefs: np.ndarray,\n",
    "    env: EnvironmentConfig,\n",
    "    demand_spec: dict,\n",
    "    *,\n",
    "    seed: int,\n",
    "    D: int,\n",
    "    reps: int\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for j in range(reps):\n",
    "        perm, visit_u, walk_u = make_common_random_draws(D, env.N, env.L, seed=seed + 100_000 + j)\n",
    "        alpha_path = make_alpha_path(D, demand_spec, np.random.default_rng(seed + 200_000 + j))\n",
    "        m = simulate_epoch_fast(agent, prefs, env, alpha_path, perm, visit_u, walk_u)\n",
    "        m = dict(m)\n",
    "        m[\"rep\"] = j\n",
    "        rows.append(m)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Gamma-first grid definition (edit freely) ---\n",
    "BASE = dict(N=600, r=0.35, chi=1.0, alpha_mean=0.35)\n",
    "\n",
    "GRID = dict(\n",
    "    gamma=[0.0, 0.4, 0.8, 1.2, 1.6, 2.0],\n",
    "    L=[3, 6, 10],\n",
    "    margin_rho_over_chi=[1.6, 2.5, 3.6],\n",
    "    discount_tau_over_rho=[0.15, 0.35, 0.7],\n",
    "    beta_concentration=[5.0, 20.0, 100.0],\n",
    ")\n",
    "\n",
    "BUDGET = dict(\n",
    "    D=30, P=70, G=26, elite_frac=0.20,\n",
    "    q_max=140, q_sigma=12.0, b_sigma=0.08,\n",
    "    oos_reps=20, preference_mode=\"correlated\",\n",
    ")\n",
    "\n",
    "OUTER_BACKEND = \"threads\"  # \"threads\" (portable) or \"processes\" (often faster on Linux/macOS)\n",
    "OUTER_WORKERS = max(1, (os.cpu_count() or 1) // 2)\n",
    "INNER_EVAL_WORKERS = 1\n",
    "\n",
    "RESULTS_CSV = OUT_DIR / \"gamma_grid_results.csv\"\n",
    "RESULTS_PARQUET = OUT_DIR / \"gamma_grid_results.parquet\"\n",
    "\n",
    "def iter_full_factorial(grid: dict) -> list[dict]:\n",
    "    keys = list(grid.keys())\n",
    "    combos = []\n",
    "    def rec(i, cur):\n",
    "        if i == len(keys):\n",
    "            combos.append(cur.copy())\n",
    "            return\n",
    "        k = keys[i]\n",
    "        for v in grid[k]:\n",
    "            cur[k] = v\n",
    "            rec(i+1, cur)\n",
    "    rec(0, {})\n",
    "    return combos\n",
    "\n",
    "combos = iter_full_factorial(GRID)\n",
    "print(\"Total combinations:\", len(combos))\n",
    "print(\"Outer backend:\", OUTER_BACKEND, \"| workers:\", OUTER_WORKERS)\n",
    "combos[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Scenario runner + checkpointed grid execution ---\n",
    "def make_env_and_demand(combo: dict):\n",
    "    L = int(combo[\"L\"])\n",
    "    margin = float(combo[\"margin_rho_over_chi\"])\n",
    "    disc = float(combo[\"discount_tau_over_rho\"])\n",
    "    conc = float(combo[\"beta_concentration\"])\n",
    "    rho = margin * BASE[\"chi\"]\n",
    "    tau = disc * rho\n",
    "\n",
    "    env = EnvironmentConfig(\n",
    "        N=int(BASE[\"N\"]),\n",
    "        L=L,\n",
    "        r=float(BASE[\"r\"]),\n",
    "        chi=float(BASE[\"chi\"]),\n",
    "        rho=float(rho),\n",
    "        tau=float(tau),\n",
    "    )\n",
    "    demand_spec = {\"kind\":\"beta\", \"alpha_mean\": float(BASE[\"alpha_mean\"]), \"concentration\": float(conc)}\n",
    "    return env, demand_spec\n",
    "\n",
    "def scenario_key(combo: dict) -> str:\n",
    "    return \"|\".join([\n",
    "        f\"g={combo['gamma']}\",\n",
    "        f\"L={combo['L']}\",\n",
    "        f\"m={combo['margin_rho_over_chi']}\",\n",
    "        f\"d={combo['discount_tau_over_rho']}\",\n",
    "        f\"c={combo['beta_concentration']}\",\n",
    "    ])\n",
    "\n",
    "def run_one_combo(combo: dict, seed: int) -> dict:\n",
    "    env, demand_spec = make_env_and_demand(combo)\n",
    "    gamma = float(combo[\"gamma\"])\n",
    "\n",
    "    out_t = evolutionary_optimize(\n",
    "        env, demand_spec, gamma=gamma, allow_tgtg=True,\n",
    "        preference_mode=BUDGET[\"preference_mode\"], seed=seed,\n",
    "        D=BUDGET[\"D\"], P=BUDGET[\"P\"], G=BUDGET[\"G\"], elite_frac=BUDGET[\"elite_frac\"],\n",
    "        q_max=BUDGET[\"q_max\"], q_sigma=BUDGET[\"q_sigma\"], b_sigma=BUDGET[\"b_sigma\"],\n",
    "        n_workers_eval=INNER_EVAL_WORKERS\n",
    "    )\n",
    "    out_b = evolutionary_optimize(\n",
    "        env, demand_spec, gamma=gamma, allow_tgtg=False,\n",
    "        preference_mode=BUDGET[\"preference_mode\"], seed=seed,\n",
    "        D=BUDGET[\"D\"], P=BUDGET[\"P\"], G=BUDGET[\"G\"], elite_frac=BUDGET[\"elite_frac\"],\n",
    "        q_max=BUDGET[\"q_max\"], q_sigma=BUDGET[\"q_sigma\"], b_sigma=BUDGET[\"b_sigma\"],\n",
    "        n_workers_eval=INNER_EVAL_WORKERS\n",
    "    )\n",
    "\n",
    "    oos_seed = seed + 777\n",
    "    oos_t = out_of_sample_eval(out_t[\"best_agent\"], out_t[\"prefs\"], env, demand_spec, seed=oos_seed, D=BUDGET[\"D\"], reps=BUDGET[\"oos_reps\"])\n",
    "    oos_b = out_of_sample_eval(out_b[\"best_agent\"], out_b[\"prefs\"], env, demand_spec, seed=oos_seed, D=BUDGET[\"D\"], reps=BUDGET[\"oos_reps\"])\n",
    "\n",
    "    row = dict(combo)\n",
    "    row[\"key\"] = scenario_key(combo)\n",
    "    row[\"seed\"] = seed\n",
    "\n",
    "    row[\"waste_base\"] = float(oos_b[\"waste\"].mean())\n",
    "    row[\"waste_tgtg\"] = float(oos_t[\"waste\"].mean())\n",
    "    row[\"delta_waste\"] = row[\"waste_tgtg\"] - row[\"waste_base\"]\n",
    "\n",
    "    row[\"profit_base\"] = float(oos_b[\"mean_profit\"].mean())\n",
    "    row[\"profit_tgtg\"] = float(oos_t[\"mean_profit\"].mean())\n",
    "    row[\"delta_profit\"] = row[\"profit_tgtg\"] - row[\"profit_base\"]\n",
    "\n",
    "    row[\"prod_base\"] = float(oos_b[\"production\"].mean())\n",
    "    row[\"prod_tgtg\"] = float(oos_t[\"production\"].mean())\n",
    "    row[\"delta_prod\"] = row[\"prod_tgtg\"] - row[\"prod_base\"]\n",
    "\n",
    "    row[\"b_star\"] = float(out_t[\"best_agent\"].b)\n",
    "\n",
    "    D = float(BUDGET[\"D\"])\n",
    "    row[\"waste_base_per_day\"] = row[\"waste_base\"] / D\n",
    "    row[\"waste_tgtg_per_day\"] = row[\"waste_tgtg\"] / D\n",
    "    row[\"delta_waste_per_day\"] = row[\"delta_waste\"] / D\n",
    "\n",
    "    row[\"prod_base_per_day\"] = row[\"prod_base\"] / D\n",
    "    row[\"prod_tgtg_per_day\"] = row[\"prod_tgtg\"] / D\n",
    "    row[\"delta_prod_per_day\"] = row[\"delta_prod\"] / D\n",
    "    return row\n",
    "\n",
    "def load_existing_results(path_csv: Path) -> pd.DataFrame:\n",
    "    if path_csv.exists():\n",
    "        return pd.read_csv(path_csv)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_results(df: pd.DataFrame):\n",
    "    df.to_csv(RESULTS_CSV, index=False)\n",
    "    try:\n",
    "        df.to_parquet(RESULTS_PARQUET, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def run_grid(combos: list[dict], seed_base: int = 3000, resume: bool = True, flush_every: int = 5) -> pd.DataFrame:\n",
    "    existing = load_existing_results(RESULTS_CSV) if resume else pd.DataFrame()\n",
    "    done = set(existing[\"key\"].astype(str).tolist()) if (resume and len(existing) > 0) else set()\n",
    "    pending = [c for c in combos if scenario_key(c) not in done]\n",
    "    print(f\"Total combos: {len(combos)} | already done: {len(done)} | pending: {len(pending)}\")\n",
    "    if len(pending) == 0:\n",
    "        return existing\n",
    "\n",
    "    rows = []\n",
    "    t0 = time.time()\n",
    "    Executor = ThreadPoolExecutor if OUTER_BACKEND == \"threads\" else ProcessPoolExecutor\n",
    "\n",
    "    with Executor(max_workers=OUTER_WORKERS) as ex:\n",
    "        futures = {}\n",
    "        for idx, combo in enumerate(pending):\n",
    "            seed = seed_base + idx * 17\n",
    "            futures[ex.submit(run_one_combo, combo, seed)] = combo\n",
    "\n",
    "        for j, f in enumerate(as_completed(futures), 1):\n",
    "            rows.append(f.result())\n",
    "            if j % flush_every == 0:\n",
    "                df_new = pd.DataFrame(rows)\n",
    "                df_all = pd.concat([existing, df_new], ignore_index=True).drop_duplicates(subset=[\"key\"], keep=\"first\")\n",
    "                save_results(df_all)\n",
    "                elapsed = time.time() - t0\n",
    "                print(f\"{j}/{len(pending)} completed | elapsed {elapsed:.1f}s | checkpoint saved\")\n",
    "\n",
    "    df_new = pd.DataFrame(rows)\n",
    "    df_all = pd.concat([existing, df_new], ignore_index=True).drop_duplicates(subset=[\"key\"], keep=\"first\")\n",
    "    save_results(df_all)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c851ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RUN (resumable)\n",
    "df = run_grid(combos, seed_base=3000, resume=True, flush_every=3)\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Gamma-centric plots ---\n",
    "def plot_gamma_curves(df: pd.DataFrame, y_col: str, group_col: str, title: str):\n",
    "    fig, ax = plt.subplots(figsize=(8.2, 4))\n",
    "    for gval in sorted(df[group_col].unique()):\n",
    "        sub = df[df[group_col] == gval].groupby(\"gamma\")[y_col].mean().reset_index().sort_values(\"gamma\")\n",
    "        ax.plot(sub[\"gamma\"], sub[y_col], marker=\"o\", label=f\"{group_col}={gval}\")\n",
    "    ax.axhline(0, linewidth=1)\n",
    "    ax.set_xlabel(\"gamma\")\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gamma_curves(df, \"delta_waste_per_day\", \"beta_concentration\", \"Δ waste/day vs γ (lines by volatility)\")\n",
    "plot_gamma_curves(df, \"delta_prod_per_day\", \"beta_concentration\", \"Δ production/day vs γ (lines by volatility)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.2, 4))\n",
    "sub = df.groupby(\"gamma\")[\"b_star\"].mean().reset_index().sort_values(\"gamma\")\n",
    "ax.plot(sub[\"gamma\"], sub[\"b_star\"], marker=\"o\")\n",
    "ax.set_xlabel(\"gamma\")\n",
    "ax.set_ylabel(\"mean b*\")\n",
    "ax.set_title(\"Mean b* vs γ (averaged over other parameters)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Conditional gamma slice ---\n",
    "L_fix = 6\n",
    "margin_fix = 2.5\n",
    "disc_fix = 0.35\n",
    "\n",
    "sl = df[(df[\"L\"] == L_fix) & (df[\"margin_rho_over_chi\"] == margin_fix) & (df[\"discount_tau_over_rho\"] == disc_fix)].copy()\n",
    "print(\"Slice rows:\", len(sl))\n",
    "\n",
    "plot_gamma_curves(sl, \"delta_waste_per_day\", \"beta_concentration\", f\"Δ waste/day vs γ | L={L_fix}, m={margin_fix}, d={disc_fix}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.2, 4))\n",
    "for conc in sorted(sl[\"beta_concentration\"].unique()):\n",
    "    sub = sl[sl[\"beta_concentration\"] == conc].groupby(\"gamma\")[\"b_star\"].mean().reset_index().sort_values(\"gamma\")\n",
    "    ax.plot(sub[\"gamma\"], sub[\"b_star\"], marker=\"o\", label=f\"conc={conc}\")\n",
    "ax.set_xlabel(\"gamma\")\n",
    "ax.set_ylabel(\"b*\")\n",
    "ax.set_title(f\"b* vs γ | L={L_fix}, m={margin_fix}, d={disc_fix}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bade0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Interaction models including gamma ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "FEATURES = [\"gamma\", \"L\", \"margin_rho_over_chi\", \"discount_tau_over_rho\", \"beta_concentration\"]\n",
    "TARGET = \"delta_waste_per_day\"  # or \"waste_tgtg_per_day\" or \"b_star\"\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "lin = Pipeline(steps=[\n",
    "    (\"poly\", PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"ridge\", Ridge(alpha=1.0))\n",
    "])\n",
    "lin.fit(X_train, y_train)\n",
    "pred_lin = lin.predict(X_test)\n",
    "print(\"Linear interactions R2:\", r2_score(y_test, pred_lin))\n",
    "print(\"Linear interactions MAE:\", mean_absolute_error(y_test, pred_lin))\n",
    "\n",
    "poly = lin.named_steps[\"poly\"]\n",
    "ridge = lin.named_steps[\"ridge\"]\n",
    "terms = poly.get_feature_names_out(FEATURES)\n",
    "coef_df = pd.DataFrame({\"term\": terms, \"coef\": ridge.coef_}).sort_values(\"coef\", key=lambda s: np.abs(s), ascending=False)\n",
    "coef_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(n_estimators=700, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "print(\"RF R2:\", r2_score(y_test, pred_rf))\n",
    "print(\"RF MAE:\", mean_absolute_error(y_test, pred_rf))\n",
    "\n",
    "imp = pd.DataFrame({\"feature\": FEATURES, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export\n",
    "df.to_csv(OUT_DIR / \"gamma_grid_results_FINAL.csv\", index=False)\n",
    "try:\n",
    "    df.to_parquet(OUT_DIR / \"gamma_grid_results_FINAL.parquet\", index=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "OUT_DIR\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
